{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af43fe-955d-4165-bad0-b655a37ddfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c65826-fecb-4196-8085-d1de52d6b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab2030c-7d4e-4482-b097-3ded8c0745e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b04ac0-071b-47be-8ba8-f899b9f43d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f07359-b43a-4dc5-94f8-5ce2af8e26ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.jupyter import print\n",
    "from rich.pretty import pprint\n",
    "from typing import List, Set\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85149416-6585-498b-9e76-0a924b7b170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a77232-e60e-43e5-aca1-88ea030c0f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Spacy version:\", spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4fb2ae-16b5-483d-8c8a-818cdb139f76",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Finding \"Tom Swifties\": Part of Speech Puns in Sci-Fi Novellas of the 1910s\n",
    "\n",
    "*Using computational linguistics to list pun candidates from book texts*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed182728-37e0-4064-aad1-69d4b584ffde",
   "metadata": {
    "editable": false,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Starting from 1910, Howard Garis wrote a series of popular Sci-Fi Adventure novellas under the pen name of Victor Appleton. The main character was [Tom Swift](https://en.wikipedia.org/wiki/Tom_Swift), a young inventor living in upstate New York. \n",
    "\n",
    "<img src=\"TomSwiftMotorcycleSmallCropped.jpg\" width=\"200\" align=\"right\" style=\"margin-left: 1em;\">\n",
    "\n",
    "Litovkina (2014) analyzed a pun construction which Garis employed prolifically in the series. She classifed these puns as a type of \"[Wellerism](https://en.wikipedia.org/wiki/Wellerism)\", a comedic device in discourse. Garis' frequent use of his specific sub-type of Wellerism helped his puns to become known as \"Tom Swifties\". Litovkina defines a Tom Swifty as:\n",
    "\n",
    "> a wellerism conventionally based on the punning relationship between the way an adverb describes a speaker and simultaneously refers to the meaning of the speaker’s statement. The speaker is traditionally Tom, his statement is usually placed at the beginning of the Tom Swifty, and the adverb at the end of it, e.g. “I see,” said Tom icily (icily vs. I see).\n",
    "\n",
    "---\n",
    "\n",
    "T. Litovkina, A. (2014). “I see,” said Tom icily: Tom Swifties at the beginning of the 21st century. *The European Journal of Humour Research*, 2(2), 54-67. https://doi.org/10.7592/EJHR2014.2.2.tlitovkina"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7687b6a5-072f-469d-a863-c5809f81687e",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Goal\n",
    "\n",
    "Litkovina doesn't mention that she herself compiled the data she analyzed. Instead, she consulted compilations frp, the internet which she lists on the last page of her paper. Many of the links no longer work. http://www.fun-with-words.com/tom_swifties_a-e.html is a decent but seemingly small compilation of the puns, alphabetized by adverbial phrase. But it's unclear who compiled them, how they were found or if theirs is a definitive list or a selection thereof. \n",
    "\n",
    "To answer that question, we will conduct our own search for *Tom Swifties* with tools from computational linguistics. \n",
    "\n",
    "<!-- We'll ttempt to compile a full set of  from a corpus of the novellas themselves.  -->\n",
    "\n",
    "<!-- We'll eventually produce a list of Tom Swifty candidates from a selection portion of the text after training a model to classify sentences as either containing a Tom Swifty or not containing one.  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be7eee-6a98-4dca-a546-9ca763bee407",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Start the Search\n",
    "\n",
    "To begin the search, we'll build a corpus of documents and preprocess the raw text to remove Gutenberg biolerplate, blank lines and table of contents since TOCs usually don't contain puns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b8057-1ee1-4520-8888-e3e8a7078f0d",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Building the Corpus\n",
    "\n",
    "We'll download the first 5 novellas from Project Gutenburg; All were published in 1910:\n",
    "\n",
    "- Tom Swift and His Motor Cycle (Gutenbeg id = 4230)\n",
    "- Tom Swift and His Motor Boat (id = 2273)\n",
    "- Tom Swift and His Airship (id = 3005)\n",
    "- Tom Swift and His Submarine Boat (id = 949)\n",
    "- Tom Swift and His Electric Runabout (id = 950)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d344b44-57d5-4f9b-bdce-90b827a15045",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's begin with our custom class, `GutenbergTextProvider`. There are some existing packages which clean texts from Project Gutenberg, but one requires installing a database and the other uses NLTK, so let's just keep it simple for today.\n",
    "\n",
    "The `fetch()` function in the class internally preprocesses the text before storing it in a class property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0df9e51-28fd-409f-94ff-ef1bf7c56bb1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.tomswiftr.text_provider import GutenbergTextProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c259c4-d951-4878-9899-c2012c63be96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_source_ids = [\n",
    "    4230,\n",
    "    2273,\n",
    "    3005,\n",
    "    949,\n",
    "    950,\n",
    "]\n",
    "\n",
    "provider = GutenbergTextProvider()\n",
    "\n",
    "# Let's start with just the first novella to test things out.\n",
    "text = provider.fetch(4230)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b20f84-d126-4ae7-b758-4b5f8c5b45e8",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Dipping our toes into the waters\n",
    "\n",
    "Let's examine the first paragraph or so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de4d9dd-9721-4300-866b-e0b3e47a51b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "provider.get_text(to_char=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f19121-0256-4e53-b5fa-55812bac995e",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "That looks fairly good; looks like we got part of the chapter title included, but I judge that this won't impede our pursuit of Tom Swifties, so let's move on. \n",
    "\n",
    "Now, let's convert this text into an array of sentences with the help of SpaCy and inspect just the first 10 for sanity's sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b9fb51-bc22-44ee-951d-7d98902c029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Doc, Language\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp: Language = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\n",
    "    \"sentencizer\"\n",
    ")  # key component. Performs far better after loading a statistical model, like en_core_web_sm.\n",
    "\n",
    "doc: Doc = nlp(provider.get_text(to_char=1000))\n",
    "\n",
    "# doc.sents  # it's a generator, yeah!\n",
    "for idx, s in enumerate(doc.sents):\n",
    "    print(f\"[{idx}] {s}\", soft_wrap=True)\n",
    "    if idx == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd6b4d-6874-40a7-b6be-c3d8efcd8e70",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Sentence **7** has the shape we might expect for a Tom Swiftie candidate. We notice thet SpaCy wil separate sentences within quotes, but we don't expect many candidates to fit that shape, if any at all. Let's restrict our search to only sentences starting with a quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa9eaf-443c-4954-a25c-4995bf9a5fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the entire book's text\n",
    "doc: Doc = nlp(provider.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4a2db-b96f-4df4-8181-b015c5faa5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc.sents: Iterator[Span]\n",
    "# s: spacy.tokens.span.Span\n",
    "for idx, s in enumerate(doc.sents):\n",
    "    if s.text.startswith('\"'):\n",
    "        print(f\"[{s.start}-{s.end}] {s}\", soft_wrap=True)\n",
    "\n",
    "        if idx == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37fbeb0-1f67-4e85-acef-a647e83a934c",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "OK, that's great and all, but let's see what named entities we can find in the same span. We're still treading gingerly for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8582e-e03d-46c2-af03-f1b4f6b9b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, s in enumerate(doc.sents):\n",
    "    if s.text.startswith('\"'):\n",
    "        print(f\"[{s.start}-{s.end}] {s}\", soft_wrap=True)\n",
    "\n",
    "        # Span.ents = The named entities that fall completely within the span.\n",
    "        #  Returns a tuple of Span objects.\n",
    "        print(s.ents)\n",
    "\n",
    "        if idx == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09fdf92-a848-42a3-a2b4-328f1d54bed0",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "`Whoop` as a named entity? That's a little embarrasing. Well, we can restrict ourselves to only sentences where one of the named entities is Tom, at least this time, so let's do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6929f0-8c45-45b4-920c-409c3b7162f7",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Candidates where Tom is the speaker\n",
    "\n",
    "But how can we ensure we're targeting the clause outside the quote? We need to rely on dependency parsing and find only sentences where \"Tom\" is the nsubj of the clause's speech verb. Luckily, SpaCy's Sentencizer does exactly that. Let's access that parse by examining the tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b94f05-8598-41d2-b51c-a6e22862fc1d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO use filter() instead?\n",
    "for idx, s in enumerate(doc.sents):\n",
    "    if s.text.startswith('\"'):\n",
    "        for ne in s.ents:\n",
    "            if \"Tom\" in ne.text:\n",
    "                print(f\"[{s.start}-{s.end}] {s}\", soft_wrap=True)\n",
    "                pprint(s.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9baef2-b781-46f8-a9ea-85b88262048b",
   "metadata": {},
   "source": [
    "Upon visual inspection, I'd say the Span of token 1178:1202 presents a great consutrction for us to look at more closely. It's clearly not a pun but we should be able to code against this pattern of verbal root with Tom as subject and an adverb dependent on that speech verb performed by Tom. \n",
    "> \"He thinks he can run over everything since he got his new auto,\" commented Tom aloud as he rode on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f145ad9f-cea0-4c63-8e28-5df07d7b83c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[1178:1202]:\n",
    "    print(\n",
    "        token.text,\n",
    "        token.pos_,\n",
    "        token.tag_,\n",
    "        token.dep_,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fe8d83-8a0e-4515-bc41-18022ac4746e",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Well, would you look at that! `commented VERB VBD ROOT xxxx`; It even parsed the dependencies and singled out the speech verb we want to target as **ROOT**. \"Ain't that grand\" said Michael largely.\n",
    "\n",
    "Perfect! almost...except that we can see ourselves that it's a speech verb, but our code doesn't know that. Let's change that.\n",
    "\n",
    "Do note however, that there are some oddities which we'll need to deal with soon enough:\n",
    "\n",
    "- Tom PROPN NNP compound Xxx\n",
    "- aloud NOUN NN nsubj xxxx\n",
    "\n",
    "`aloud` should be an adverb and we dearly need it to be in this exploration. We also need `Tom` to be the nsubj. Yet, SpaCy thinks `Tom` is a compund and categorizes `aloud` as the nsubj!\n",
    "\n",
    "Let's examine a diagram of this in two forms; one is a traditional dependency arc graph and the other merely highlights the named entities in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d329e8-aca7-48d9-ac40-4e84286f084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44746d62-3f8e-4ab2-bf47-2cf59031f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc[1178:1202], style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e7623",
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc[1178:1202], style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41a1ecc-5980-40b1-be78-11ca032807d5",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Narrowing the search\n",
    "\n",
    "We're getting closer, even though the above example is not comforting. Let's start implementing code to only search for the pattern we really want. Let's start with the root verb.\n",
    "\n",
    "> the most frequently used verbs introducing the text of the statement in Tom Swifties is said. (57)\n",
    "\n",
    "> Nevertheless, there are a number\n",
    "of other verbs employed for this purpose as well, e.g.: admitted, agreed, asked, bemoaned,\n",
    "considered, consented, cried, debated, decided, discovered, guessed, implied, mused, nagged,\n",
    "pleaded, pretended, professed, queried, recounted, remarked, replied, revealed, reviewed,\n",
    "sang, and yelled. (57)\n",
    "\n",
    "Our first pattern will be to find sentences with a quotation followed by the named entity, \"Tom\" (his last name seems rarely used in the text) and a verb denoting a speech act linked to an adverb. Note that the verb is not always as explicitly denoting speech as *said* does; it may be slightly parenthentical as well, like *guessed*.\n",
    "\n",
    "As such, the verb for which \"Tom Swift\" is a *nsubj* and which the adverb modifies, will be one of those verbs listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bb8397-c9c4-4517-b0d6-bd736656bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_verbs: Set = {\n",
    "    \"admitted\",\n",
    "    \"agreed\",\n",
    "    \"asked\",\n",
    "    \"bemoaned\",\n",
    "    \"considered\",\n",
    "    \"consented\",\n",
    "    \"cried\",\n",
    "    \"debated\",\n",
    "    \"decided\",\n",
    "    \"discovered\",\n",
    "    \"guessed\",\n",
    "    \"implied\",\n",
    "    \"mused\",\n",
    "    \"nagged\",\n",
    "    \"pleaded\",\n",
    "    \"pretended\",\n",
    "    \"professed\",\n",
    "    \"queried\",\n",
    "    \"recounted\",\n",
    "    \"remarked\",\n",
    "    \"replied\",\n",
    "    \"retored\",\n",
    "    \"revealed\",\n",
    "    \"reviewed\",\n",
    "    \"said\",\n",
    "    \"sang\",\n",
    "    \"yelled\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dacde9-a0b6-4038-8be5-b565cec9d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = []\n",
    "\n",
    "\n",
    "for idx, s in enumerate(doc.sents):\n",
    "    qualifies = False\n",
    "\n",
    "    if s.text.startswith('\"'):\n",
    "        # TODO the Span class has a .root property pointing to the root token\n",
    "\n",
    "        # TODO if any(target_element in tup for tup in list_of_tuples):\n",
    "        for ne in s.ents:\n",
    "            if \"Tom\" in ne.text:\n",
    "                # print(f\"[{s.start}-{s.end}] {s}\", soft_wrap=True)\n",
    "                qualifies = True\n",
    "                continue\n",
    "\n",
    "    if qualifies:\n",
    "        # Now search for a speech verb which is the root of the dependency parse\n",
    "        for token in doc[s.start : s.end]:\n",
    "            # pprint(token, expand_all=True)\n",
    "\n",
    "            if (\n",
    "                token.pos_ == \"VERB\"\n",
    "                and token.tag_ == \"VBD\"  # should be past tense\n",
    "                and token.dep_ == \"ROOT\"  # must be root\n",
    "                and token.text in speech_verbs\n",
    "            ):\n",
    "                # sentence qualifies; add it and move on to the next sent\n",
    "                candidates.append(s)\n",
    "                continue\n",
    "\n",
    "print(len(candidates))\n",
    "pprint(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b898262-d328-4a74-ba3b-8351fcf43317",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "99 candidates! Not a bad list, but it's clear as we restrict our list more tightly, that number is likely to go way down.\n",
    "\n",
    "So then, let's finally return candidate sentences which have that adverb modifying the root speech verb which we've mentioned a few times already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c6b2a-53ae-4b47-855b-90dc2a95dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, s in enumerate(doc.sents):\n",
    "    if s.text.startswith('\"'):\n",
    "        root_token = s.root\n",
    "\n",
    "        if (\n",
    "            root_token.pos_ == \"VERB\"\n",
    "            and root_token.tag_ == \"VBD\"\n",
    "            and root_token.text in speech_verbs\n",
    "        ):\n",
    "            subj, adv = None, None\n",
    "            for k, child in enumerate(root_token.children):\n",
    "                # Does the root have a subj and is that subj Tom?\n",
    "                if (\n",
    "                    child.dep_ == \"nsubj\"\n",
    "                    and child.pos_ == \"PROPN\"\n",
    "                    and child.text == \"Tom\"\n",
    "                ):\n",
    "                    # tag_ is NNP\n",
    "                    subj = child\n",
    "\n",
    "                # Furthermore, Does the root have a child adverbial?\n",
    "                if child.dep_ == \"advmod\":\n",
    "                    adv = child\n",
    "\n",
    "            if subj and adv:\n",
    "                print(f\"---------{i} is a candidate---------\")\n",
    "                print(subj, adv)\n",
    "                print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9f7aa-a5d6-4b40-acab-d1b8e3d8e2cb",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "I find it hard to believe that there's only 10 instances in the whole book. Maybe we should note the above findings regarding `compound` and check those. Noticeably, a sentence which should qualify from our Great List of 99 above is missing from our list of candidates:\n",
    "\n",
    "> \"Oh, it's just a knack,\" replied Tom modestly.\n",
    "\n",
    "Why?\n",
    "\n",
    "Let's parse out this exact sentence from scratch and maybe we will find a clue leading us to the villian of the story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7d216a-a715-4606-a4ed-5f6027304c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2: Doc = nlp('\"Oh, it\\'s just a knack,\" replied Tom modestly.')\n",
    "\n",
    "# raw text\n",
    "print(doc2)\n",
    "displacy.render(doc2, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa8afcd-db4c-48f1-837e-edf27415f421",
   "metadata": {},
   "source": [
    "Oh, it thinks Tom is npadvmod, a \"noun phrase as adverbial modifier\"...ummmm...what are the POS tags, though?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39da0d40-b9c6-42bb-8ec8-bc7d5b60ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc2.to_json())\n",
    "\n",
    "for token in doc2:\n",
    "    print(\n",
    "        token.text,\n",
    "        token.pos_,\n",
    "        token.tag_,\n",
    "        token.dep_,\n",
    "        token.head,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95768d9-6d1c-4f4f-acc4-87be9d946f6d",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The POS looks right for the verb, for \"Tom\" and \"modestly\":\n",
    "\n",
    "- replied VERB VBD ROOT replied\n",
    "- Tom PROPN NNP npadvmod replied\n",
    "- modestly ADV RB advmod replied\n",
    "\n",
    "Maybe I should rely less on the dependency parse? Perhaps some complicated code relying on a combination of dependencies and POS and/or tags? But, shouldn't code be elegantly simple and not headache-inducing? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8d3b2-7d0b-47d1-ae21-0b856ddbf80e",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## A Premature Ending or, \"Evaluating the Model\"\n",
    "\n",
    "So far, we've found no puns in the first book even from visual inspection. I'm reluctant to continue farther since it's clear that SpaCy's dependency parse is lacking. The Github issues are filled with tickets mentioning this. These reports state that no matter which size of the model (`en_core_web_sm`, `en_core_web_md` or `en_core_web_lg`), the deficencies remain.\n",
    "\n",
    "I can only imagine Tom Swift dusting himself off and staying in his lab or taking to the skies to experiment ceaselessly, so in that spirit, let's discuss my thoughts on future directions for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f28b5d1-400f-4143-9499-2d8a85aed14f",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Future Follow-ons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0f4174-8dab-429c-9098-e307eb368169",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Evolution over time\n",
    "\n",
    "Garis ghost-wrote the Tom Swift series until 1935, producing 35 novellas. I'd like to expand this exploration to analyze his output of Tom Swifties over that long timespan. \n",
    "\n",
    "- Does the frequency of Tom Swifty usage change over that 25 year span?\n",
    "- Did the grammatical shape of these puns change over time?\n",
    "\n",
    "So far, I wonder if he started this trend with later books? One might try by repeating the above process with some of the other Gutenberg IDs listed near the start of this exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3998e981-0dbf-4bb4-ae77-e9347652273c",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Candidates where someone else is the speaker\n",
    "\n",
    "> Traditionally Tom is the speaker, but this is by no means necessary for the pun to classify as a Tom Swifty. Sometimes the pun lies in the name, in which case it will usually not be Tom speaking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205f095c-087f-4de6-8077-b7e9bddf45c9",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### NLP to explain the puns (when found!)\n",
    "\n",
    "Litovkin states that most adverbs inTom Swifties have either paronyms or homonyms (56). A paronym describes \"words which are linked by a similarity of form\" (Oxford Concise Dictionary of Linguistics, p288)\n",
    "\n",
    "She also says that,\n",
    "\n",
    "> either explicitly used in the statement of it or only implicitly implied. (56)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- #### Finding Paronyms -->\n",
    "<!-- A paronym is <???> -->\n",
    "\n",
    "<!-- #### Finding Homonyms -->\n",
    "<!-- A homonym is <???> -->\n",
    "\n",
    "<!-- #### Finding Homophones -->\n",
    "<!-- A homophone is <???> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeec0aa-7587-4a6c-96ae-364bbb0f7da8",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "1. Litkovina mistakenly attributes Stratemeyer as the ghostwriter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
